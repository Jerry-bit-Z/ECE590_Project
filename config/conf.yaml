init_param: ["/zpool-00/data/jerryzhang/SW/FunCodec/egs/LibriTTS/text2speech_laura/exp/audio_codec-encodec-zh_en-general-16k-nq32ds640-pytorch/model.pth:quantizer.rq.model:quantizer_codebook"]
train_shape_file: ["/zpool-00/data/jerryzhang/SW/FunCodec/egs/LibriTTS/text2speech_laura/libritts_states/train/codec_shape"]
valid_shape_file: ["/zpool-00/data/jerryzhang/SW/FunCodec/egs/LibriTTS/text2speech_laura/libritts_states/dev/codec_shape"]
train_data_path_and_name_and_type: [
    [
        "/zpool-00/data/jerryzhang/SW/FunCodec/egs/LibriTTS/text2speech_laura/dump/libritts/train/phoneme",
        "text",
        "text"
    ],
    [
        "/zpool-00/data/jerryzhang/SW/FunCodec/egs/LibriTTS/text2speech_laura/dump/libritts/train/codec_token.scp",
        "codec",
        "kaldi_ark"
    ]
]
valid_data_path_and_name_and_type: [
    [
        "/zpool-00/data/jerryzhang/SW/FunCodec/egs/LibriTTS/text2speech_laura/dump/libritts/dev/phoneme",
        "text",
        "text"
    ],
    [
        "/zpool-00/data/jerryzhang/SW/FunCodec/egs/LibriTTS/text2speech_laura/dump/libritts/dev/codec_token.scp",
        "codec",
        "kaldi_ark"
    ]
]

grad_clip: 5
token_list: "/zpool-00/data/jerryzhang/SW/FunCodec/egs/LibriTTS/text2speech_laura/data/en_phoneme_token.list"
token_type: "word"
seed: 1234
init: null

# input related
input_size: 1536
use_preprocessor: true
audio_max_duration: 60
codec_token_rate: 25

# network architecture
# encoder related
text_encoder: conformer
text_encoder_conf:
    output_size: 512    # dimension of attention
    attention_heads: 8
    linear_units: 2048  # the number of units of position-wise feed forward
    num_blocks: 6      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.0
    input_layer: linear # encoder architecture type
    normalize_before: true
    rel_pos_type: latest
    pos_enc_layer_type: rel_pos
    selfattention_layer_type: rel_selfattn
    use_cnn_module: false

# decoder related
codec_encoder: conformer
codec_encoder_conf:
    output_size: 512    # dimension of attention
    attention_heads: 8
    linear_units: 2048  # the number of units of position-wise feed forward
    num_blocks: 6      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.0
    input_layer: linear # encoder architecture type
    normalize_before: true
    rel_pos_type: latest
    pos_enc_layer_type: rel_pos
    selfattention_layer_type: rel_selfattn
    use_cnn_module: false

# model related
model: laura_gen_model
model_conf:
    codec_sampling_ratio: 0.5
    lsm_weight: 0.0
    length_normalized_loss: true
    predict_nq: 2
    codec_conf:
        num_quantizers: 32
        codebook_size: 1024
        codebook_dim: 128
    codec_lm_conf:
        name: transformer
        pos_enc: rel_pos
        selfattention_layer_type: rel_selfattn
        embed_unit: 128
        att_unit: 512
        head: 8
        unit: 2048
        # layer: 12
        layer: 6
        dropout_rate: 0.1
        pe_type: uni
        bidirectional_inputs: true
        codec_groups: 1

### Training related
batch_type: length
batch_bins: 10240 #40960 , 20480
batch_size: 80 # This does not matter here
sort_in_batch: descending
sort_batch: descending
num_workers: 2
max_cache_size: 0.0
max_cache_fd: 32
train_dtype: float32
## Add for argument type checking
allow_variable_data_keys: false
drop_last: false
fold_length: []

optim:
    type: Adam
    args:
        lr: 1.0e-3

scheduler: warmuplr
scheduler_conf:
    warmup_steps: 10000

best_field: loss
best_save_type: descend
max_ckpt: 1
log_interval: 10
epoch: 50

# training process
# num_iters_per_epoch: 10000
grad_clip: 5

  
